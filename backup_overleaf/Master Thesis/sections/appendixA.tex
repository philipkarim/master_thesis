\documentclass[../main.tex]{subfiles}
\begin{document}

\chapter{Machine learning}
\label{sec:first-app}

\section{Source code}
All the source code is located in \href{https://github.com/philipkarim/master_thesis}{this GitHub repository}.

\section{Derivation of gradients used in backpropagation}
The derivation is done according to \cite[ch.~7]{rojasRa:nnsystematic}. Starting of with the expression of finding the gradient of the activation in the last layer with respect to some weight and with respect to some bias by applying the chain rule:

\begin{align}
     \frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial W_{i j}^{last}}&=\frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{i}^{last}} \frac{\partial a_{i}^{last}}{\partial W_{i j}^{last}}=\frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{i}^{last}} \frac{\partial a_{i}^{last}}{\partial z_{i}^{last}} \frac{\partial z_{i}^{last}}{\partial W_{i j}^{last}}\\
     \frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial b_{i}^{last}}&=\frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{i}^{last}} \frac{\partial a_{i}^{last}}{\partial z_{i}^{last}} \frac{\partial z_{i}^{last}}{\partial b_{i}^{last}}
\end{align}

Where the nodes in the foregoing layers naturally affect the output which means that the derivatives of these foregoing nodes also is needed, which gives the following expressions of the derivatives of these nodes with respect to the weights and bias:

\begin{align}
\frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial W_{i j}^{last-1}}&=\sum_{k} \frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{k}^{last}} \frac{\partial a_{k}^{last}}{\partial W_{i j}^{last-1}} \\
&=\sum_{k} \frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{k}^{last}} \frac{\partial a_{k}^{last}}{\partial z_{k}^{last}} \frac{\partial z_{k}^{last}}{\partial a_{i}^{last-1}} \frac{\partial a_{i}^{last-1}}{\partial z_{i}^{last-1}} \frac{\partial z_{i}^{last-1}}{\partial W_{i j}^{last-1}}\\
\frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial b_{i}^{last-1}}&=\sum_{k} \frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{k}^{last}} \frac{\partial a_{k}^{last}}{\partial b_{i}^{last-1}} \\
&=\sum_{k} \frac{\partial L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial a_{k}^{last}} \frac{\partial a_{k}^{last}}{\partial z_{k}^{last}} \frac{\partial z_{k}^{last}}{\partial a_{i}^{last-1}} \frac{\partial a_{i}^{last-1}}{\partial z_{i}^{last-1}} \frac{\partial z_{i}^{last-1}}{\partial b_{i}^{last-1}}
\end{align}

Then by expressing the derivative of the output layer, with respect to the activation in some random layer $l$, as a sum of the derivatives of the nodes in the following layer with respect to the activation of the same node, the following expression can be written as follows:

\begin{equation}
    z_i^l= \frac{\partial L(\boldsymbol{a}^{last},\boldsymbol{y})}{\partial a_{i}^l}= \sum_n z_n^{l+1}\frac{\partial a_n^{l+1}}{\partial a_{i}^l}
\end{equation}

Which gives the final expressions for the derivative of the output layer:

\begin{align}
\frac{L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial W_{i j}^{l}}&=z_{i}^{l} \frac{\partial a_{i}^{l}}{\partial W_{i j}^{l}} \\
\frac{L\left(\boldsymbol{a}^{last}, \boldsymbol{y}\right)}{\partial b_{i}^{l}}&=z_{i}^{l} \frac{\partial a_{i}^{l}}{\partial b_{i}^{l}}
\end{align}


\end{document}